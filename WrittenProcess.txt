Here is the general outline of my process that was previouslly my readme file

usages in order
python crawl.py https://example.com 3 2
python index.py
python query.py
python run.py

Process:
Started new project in VScode on my computer
Installed the latest version of python 3.12.3
Created a github repository for this project
- initialized this project with a license and a ReadME file
  - put the contents to be the headers given on the project info document

Started with the first objective
- installed scrapy
- created a new scrapy project
- created a spider that will take inputs URL/Domain, Max Pages, Max Depth
- makes sure spider crawled the information correctly 
- Created a python file to run the crawler
  - made sure that the user can specify each of the variables URL/Domain, Max Pages, Max Depth
  - made sure that it correctly imitates the crawler, and has correct uage requirements met (will not error out)
  - Usage: python crawl.py https://example.com 3 2
 
Moved onto second objective
- installed sklearn, pickle
- started to try and import sklearn
- ran into issues of it not recognizing the import
- spent a lot of time troubleshooting, to no avail
- decided on my own to try and downgrade my python verson from 3.12.3 to 3.10.7 as that was the 'global version' indicated on my computer and within VScode
  - so uninstalled python on computer, and reinstalled 3.10.7
- Installed sklearn, pickle
- starting to think about writing the code for the indexer. realized I needed to store the crawled documents somewhere

Went back to modify first objective
- had to reinstalled scrapy,
- recreated the project with an updated name
- decided to store the crawled documents to a folder called 'Crawled Documents'
- decided to save each crawled page in a separate html file fo easier reading
- updated naming convention to acknowledge the specific command run
- fixed the files saving to the right place

Resume with second objective 
- decided that functionality will be to take all the pages that are in the 'CrawledDocuments' Folder and index them
  - Usage: python index.py
- needed functionality, for: 
  - getting(collecting) the documents 
  - correctly parsing the documents 
  - building the actual indexes 
  - saving them in pkl format. 
- created those functionalities. 
  - needed additional imports: numpy, beautifulsoup4
  - Since I was just focusing on Indexes decided to wait to implement Cosine similarity until working on part 3 
- since there were differently stored html files I decided to follow the same format for the indexes. Each index was stories, so each file contained the 'vocabulary'(terms) and the TF-IDF matrix for each term. 
- Then decided to store a separate file created with the full TF-IDF matrix for each term within all the documents 
- Stored each of these files in a separate folder called 'IndexedDocuments' 
- Since the .pkl files where not 'readable' I wanted a way to authenticate it is working properly therefore added a separate functionality to save to txt files as well. 
- in order to not conflict with requirements these files were stored in a separate folder. 
  - Decided to keep this functionality for post-testing because I like it, and gives the user the ability to see specific documents  
 
Moved on to third objective  
- installed Flask
- looked up what flask was /how to implement since I have not used it before
  - used page: https://ask.replit.com/t/how-should-i-go-about-learning-flask/58130
    - used subsequent links on this page as well
- Attempted implementation of flask app,
- went back to the index file to create a 'search' function for the query
  - Therefore now implementing the cosine_similarity
- created a function in flask app to handle the query as well as make sure it is using the indexed documents.
- tried running flask app, realized needed other functionality to handle user input
- created another file to handle this
  - Usage: python query.py 
  - this file will run the flask app, so the user does not have to do that separately and then prompt user for a query
  - also allows the user to exit, by typing 'exit'
  - this file also will output the top ranked documents based on the Similarity score to the query (as calculated in the search function)

Fixes and Improvements 1:
- user can now specify the number of results they want retrieved y tagging the end of there query with a k number 'k=#'
  - it will default to 10 items retrieved
    - decided this because there could be ALOT of indexed documents.
- fixed the usage in the CrawlerRun document to reflect the name changes, 
- Spelling Corrections Here

Fixes and Improvements 2:
- added new py file to run the crawler then index, then the flask app in succession.
  - Usage: python run.py https://example.com 3 2
- added # type: ignore to some of the imports to get rid of warnings
- renamed CrawlerRun file to just crawl and updated usages

Fixes and Improvements 3:
- rename the files so that it is the title of the page
- renamed the overall TF_IDF file so it comes first in the folder

Future ideas:
- spelling correction/suggestions
